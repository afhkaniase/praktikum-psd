---
title: "Tugas Mandiri PSD"
author: "Arfiah Kania Sektiaruni"
date: "2023-09-26"
output: html_document
---

## Packages

```{r}
lapply(c("tidyverse","rvest","kableExtra"),library,character.only=T)[[1]]
library(glmnet)
library(imputeTS)
library(corrplot)
library(randtests)
library(lmridge)
library(lares)
library(MASS)
library(elasticnet)
library(caret)
```

## Input Data

```{r}
datapsdind <- rio::import("https://raw.githubusercontent.com/afhkaniase/praktikum-psd/main/Tugas%20Individu/Data%20World%20Happiness%20Report%202023.csv")
str(datapsdind)
head(datapsdind)
```

## Data Peubah

Analisis ini menggunakan data sekunder yang berasal dari situs kaggle.com yang mengacu pada World Happiness Report 2023. Laporan tersebut berisi data tahunan tentang skor kebahagiaan berbagai negara yang diterbitkan oleh Perserikatan Bangsa-Bangsa (PBB). Penelitian dilakukan untuk mengetahui hubungan dari beberapa peubah, yaitu PDB per kapita, harapan hidup sehat, dukungan sosial, kebebasan dalam menentukan pilihan hidup, kemurahan hati, dan persepsi korupsi (Helliwell et al. 2023). Peubah yang digunakan pada analisis ini berjumlah 6 yang terdiri dari satu peubah respon dan sebelas peubah penjelas yang disajikan pada tabel berikut.

$$
Y = Skor \ Kebahagiaan\\X1 = PDB \ per \ kapita\\X2 = Harapan \ hidup \ sehat\\X3 = Dukungan \ Sosial\\X4 = Kekebasan \ dalam \ menentukan \ pilihan \ hidup\\X5 = Kemurahan \ hati\\X6 = Persepsi \ korupsi
$$

## Data Cleaning 

```{r}
# Menghapus baris dengan nilai NA
sum(is.na(datapsdind))
```
Karena terdapat 2 data yang 'NULL' maka dilakukan interpolasi

```{r}
datapsdindnew <- na_interpolation(datapsdind, option="spline")
head(datapsdindnew)
```

```{r}
# Mengecek kembali nilai NA
sum(is.na(datapsdindnew))
```
Setelah dilakukan interpolasi, tidak ada lagi nilai NA

```{r}
y <- datapsdindnew$Y
x <- data.matrix(datapsdindnew[, c('X1', 'X2', 'X3', 'X4', 'X5', 'X6')])
```

## Eksplorasi Data

```{r}
# Sebaran peubah Y (Skor Kebahagiaan)
hist(datapsdindnew$Y, col = "skyblue")
```

```{r}
df_numeric <- datapsdindnew[sapply(datapsdindnew, is.numeric)]
cor_matrix <- cor(df_numeric, use = "complete.obs")
corrplot(cor_matrix, method = "number")
```


## Model Regresi Klasik

### Model Regresi

```{r}
model_datapsdind <- lm(Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = datapsdindnew)
summary(model_datapsdind)
```

Model regresi yang terbentuk

$$
\hat{Y_t}=-1.73785+0.23455X_1+0.02042X_2+3.47071X_3+1.97481X_4+0.15356X_5-0.78788X_6
$$

## Multikolinearitas

```{r}
car::vif(model_datapsdind)
```
Tidak terdapat peubah penjelas dengan nilai $VIF > 10$. Hal ini membuktikan bahwa tidak terdapat permasalahan multikolinearitas antar peubah

## Pengujian Asumsi

### Uji Asumsi Normalitas
```{r}
##Kolmogorov-Smirnov Test
ks.test(model_datapsdind$residuals, "pnorm", mean=mean(model_datapsdind$residuals), sd=sd(model_datapsdind$residuals))
```
Hasil uji Kolmogorov-smirnov menunjukkan bahwa data sisaan pada model regresi menyebar normal dengan $p-value > 0.05$ pada tingkat kepercayaan 95%.

### Uji Asumsi Homoskedastisitas (Gauss Markov)
```{r}
lmtest::bptest(model_datapsdind)
```
Diketahui $p-value > 0.05$ sehingga ada bukti untuk menyatakan bahwa sisaan pada model tidak terjadi gejala heteroskedastisitas pada tingkat kepercayaan 95%.

### Uji Kebebasan Sisaan (Gauss Markov)
```{r}
runs.test(model_datapsdind$residuals)
```
Diketahui $p-value > 0.05$ sehingga ada bukti untuk menyatakan bahwa sisaan pada model saling bebas (tidak ada autokorelasi) pada tingkat kepercayaan 95%

### Uji Nilai Harapan Sisaan
```{r}
t.test(model_datapsdind$residuals,
       mu = 0,
       conf.level = 0.95)
```
Diketahui $p-value > 0.05$ sehingga ada bukti untuk menyatakan bahwa nilai harapan sisaan adalah nol pada tingkat kepercayaan 95%.

### AIC

```{r}
AICklasik <- AIC(model_datapsdind)
AICklasik
```

## Pemilihan Peubah Penjelas/ Variable Selection

### BEST SUBSET

```{r}
library(leaps)
regfit.full=regsubsets(Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = datapsdindnew, nvmax=64)
reg.summary=summary(regfit.full)
reg.summary
reg.summary$adjr2
which.max(reg.summary$adjr2)
coef(regfit.full,6)

olsrr::ols_step_best_subset(model_datapsdind)
```

Pada pemilihan peubah dengan teknik best subset terlihat nilai AIC paling rendah serta Adj. R-Squared paling tinggi dimiliki oleh model ke-5. Pada model ini, peubah X5(Kemurahan hati) tidak dimasukkan ke dalam model. Nilai Adj. R-Square didapatkan sebesar 0.7983 yang artinya dapat dikatakan baik untuk menggambarkan keragaman skor kebahagiaan oleh kelima peubah yang terpilih. 

### Metode Forward

```{r}
fmodelselect <- step(lm(y ~ 1, datapsdindnew), direction="forward", scope=formula(model_datapsdind), trace=1)
summary(fmodelselect)
```

```{r}
olsrr::ols_step_forward_p(model_datapsdind)
```

Hasil metode Stepwise Forward juga menunjukkan hal yang sama. Langkah paling optimal berada pada langkah ke-5, yang mana belum memasukkan peubah X5(Kemurahan hati) ke dalam model. Langkah ke-5 ini dipilih dikarenakan memiliki nilai AIC terkecil yaitu 181.1803 dengan Adj. R-Square sebesar 0.7983.

### Metode Backward

```{r}
bmodelselect <- step(model_datapsdind, direction="backward", scope=formula(lm(Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = datapsdindnew)), trace=1)
summary(bmodelselect)
```

```{r}
olsrr::ols_step_backward_p(model_datapsdind)
```

Pada metode Stepwise Backward, peubah X5(Kemurahan hati) dikeluarkan dari model. Menunjukkan bahwa metode Best Subset, Stepwise Forward, dan Stepwise Backward menghasilkan kesimpulan yang sama yaitu: Model terbaik diperoleh dengan tidak memasukkan peubah X5(Kemurahan hati).

### Metode Stepwise

```{r}
smodelselect <- step(lm(y ~ 1, datapsdindnew), direction="both", scope=formula(model_datapsdind), trace=1)
summary(smodelselect)
```

```{r}
olsrr::ols_step_both_p(model_datapsdind)
```

## Ridge Regression

```{r}
y <- datapsdindnew$Y
x <- data.matrix(datapsdindnew[, c('X1', 'X2', 'X3', 'X4', 'X5', 'X6')])
```

#### Fungsi `glmnet`

```{r}
cv.r <- cv.glmnet(x,y,alpha=0)
plot(cv.r)
```

#### Hasil Regresi Ridge

```{r}
library(lmridge)
model.ridge<-lmridge(Y~., data=datapsdindnew)
summary(model.ridge)
```

#### Koefisien Ridge

```{r}
best_lambda <- cv.r$lambda.min
best_ridge <- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_ridge)
```

## Lasso Regression

```{r}
y <- matrix(datapsdindnew$Y)
x <- data.matrix(datapsdindnew[, c('X1', 'X2', 'X3', 'X4', 'X5', 'X6')])
```

### Fungsi `glmnet`

```{r}
cv.l<-cv.glmnet(x,y,alpha=1)
plot(cv.l)
```

### Hasil Regresi Lasso

```{r}
model.lasso<-lasso_vars(datapsdindnew,Y) 
model.lasso
```

### Koefisien Lasso

```{r}
best.ll<-cv.l$lambda.min
bestlasso<-glmnet(x,y,alpha=1,lambda=best.ll)
coef(bestlasso)
```

### Adj. R-Square Lasso

```{r}
n <- length(datapsdindnew$Y)
p <- ncol(datapsdindnew)
r_squared_lasso <- 0.8058

adjusted_r_squared <- 1 - ((1 - r_squared_lasso) * (n - 1) / (n - p - 1))
adjusted_r_squared
```

## Perbandingan Model Klasik, Ridge Regression, dan Lasso Regression
```{r}
comparison_table <- data.frame(
  Method = c("Klasik", "Best Subset", "Ridge", "Lasso"),
  AIC = c(AICklasik, 181.1803, -198.51767, 183.2031),
  Adj.R2 = c(0.7971, 0.7983, 0.7987, 0.7940),
  R2 = c(0.8063, 0.8063, 0.8063, 0.8058)
) %>%
  arrange(desc(R2))

print(comparison_table)
```

Jika dilihat dari nilai R-Square, model regresi hasil Klasik, Best Subset, dan Ridge adalah yang terbaik karena menghasilkan nilai terbaik yang sama. Namun, jika dilihat dari nilai AIC maka model Ridge adalah yang terbaik. Selisih nilai AIC model Ridge dengan ketiga model lainnya sangat besar, sedangkan perbedaan nilai R-square diantara keempat model tidak jauh berbeda. Oleh karena itu, model Ridge dipilih sebagai model yang paling baik untuk data yang digunakan.

## Peubah Yang Berpengaruh

Pada model Klasik, Best Subset, Ridge, maupun Lasso memiliki hasil yang sama. Peubah penjelas yang berpengaruh signifikan terhadap peubah respon adalah X1, X2, X3, X4, dan X6, namun 1 diantaranya (X5 = Kemurahan Hati) tidak berpengaruh signifikan terhadap peubah respon. Peubah X5 tidak dipertahankan pada model Ridge, sedangkan pada model Lasso dipertahankan. Model ini memiliki R2 sebesar 0.806. Artinya, model ini sudah menjelaskan 80.6% keberagaman dari data. Berikut adalah model yang dibentuk.

## Interpretasi Model Terbaik

```{r}
coef(best_ridge)
```

$$
\hat{Y_t}=-1.71767815+0.21793065X_1+0.02857663X_2+2.96995280X_3+1.93516469X_4+0.17185865X_5-0.73069075X_6
$$

Peubah penjelas yang signifikan terhadap peubah respon:
- Peubah X1 (PDB per kapita) adalah 0.21793065, yang berarti semakin besar PDB per kapita di suatu negara, maka perkiraan nilai Skor Kebahagiaan relatif akan semkain meningkat.

- Peubah X3 (Dukungan Sosial) adalah 2.96995280, yang berarti semakin besar Dukungan Sosial di lingkungan suatu negara tertentu, maka perkiraan nilai Skor Kebahagiaan akan semakin meningkat.

- Peubah X4 (Kekebasan dalam menentukan pilihan hidup) adalah 1.93516469, yang berarti semakin besar dukungan Kekebasan dalam menentukan pilihan hidup di lingkungan suatu negara tertentu, maka perkiraan nilai Skor Kebahagiaan relatif akan semakin meningkat.

- Peubah X6 (Persepsi korupsi) adalah -0.73069075, yang berarti bahwa semakin meningkat Persepsi korupsi di suatu negara, maka perkiraan nilai Skor Kebahagiaan akan semakin menurun.


Peubah penjelas yang tidak signifikan terhadap peubah respon:
- Peubah X2 (Harapan hidup sehat) adalah 0.02857663, yang berarti bahwa semakin besar nilai Harapan hidup sehat seseorang di suatu negara, maka perkiraan nilai Skor Kebahagiaan akan semakin meningkat.

- Peubah X5 (Kemurahan hati) adalah 0.17185865, yang berarti semakin besar rasa Kemurahan hati seseorang di lingkungan suatu negara, maka perkiraan nilai Skor Kebahagiaan akan semakin meningkat.
